---
title: Visualizing lidar data
header:
  overlay_image: images/posts/kitti-lidar/bg.jpg
  caption: "Image credit: [**extremetech**](https://www.extremetech.com/extreme/213517-a-laser-and-a-raspberry-pi-can-disable-a-self-driving-car)"
  overlay_filter: 0.5
excerpt: "Arguably the most essential piece of hardware for a self-driving car setup is a lidar. A [lidar](https://en.wikipedia.org/wiki/Lidar) allows to collect precise distances to nearby objects by continuously scanning vehicle surroundings with a beam of laser light, and measuring how long it took the reflected pulses to travel back to sensor."
tags:
- Python
- Computer vision
- ML
crosspost_to_medium: false
---
{% include toc title="Contents" icon="none" %}

Although lidars used to be the most expensive components of self-driving cars, and could easily cost you as much as $75,000 just a couple of years ago,  prices have plummeted recently and there are really good lidar sensors on the market in sub-$8000 range these days. And it just keeps getting better as Velodyne has just [announced](http://www.businesswire.com/news/home/20170419005516/en/Velodyne-LiDAR-Announces-%E2%80%9CVelarray%E2%80%9D-LiDAR-Sensor) a whole magnitude cheaper model range with a limited field-of-view, presumably costing just under $1000.

# Dataset
Luckily, you don't have to spend that much money to get hold of data generated by a lidar. [KITTI Vision Benchmark Suite](http://www.cvlibs.net/datasets/kitti/) contains datasets collected with a car driving around rural areas of a city — a car equipped with a lidar and a bunch of cameras, of course. Some of those datasets are labeled, e.g. they also contain information about objects around it; we will visualize those as well. These datasets are [publicly available here](http://www.cvlibs.net/datasets/kitti/raw_data.php), if you would like to follow along just go ahead and download one of them. 

I will use the `2011_09_26_drive_0001` dataset and corresponding tracklets, e.g. labeled surrounding objects. It is one of the smallest datasets out there (0.4 GB) which contains data for just 11 seconds of driving:

* **Length**: 114 frames (00:11 minutes)
* **Image resolution**: `1392 x 512` pixels
* **Labels**: 12 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 1 Trams, 0 Misc

# Dependencies

A lidar operates by streaming a laser beam at high frequencies, generating a 3D point cloud as an output in realtime. We are going to use a couple of dependencies to work with the point cloud presented in the KITTI dataset: apart from the familiar toolset of `numpy` and `matplotlib` we will use [`pykitti`](https://github.com/utiasSTARS/pykitti). In order to make tracklets parsing math easier we will use a couple of methods originally implemented by Christian Herdtweck that I have updated for Python 3, you can find them in `source/parseTrackletXML.py` in the project repo.

# Visualization

## Cameras

In addition to the lidar 3D point cloud data KITTI dataset also contains video frames from a set of forward facing cameras mounted on the vehicle. The regular camera data is not half as exciting as the lidar data, but is still worth checking out.

![image-center]({{ base_path }}/images/posts/kitti-lidar/cameras.png){: .align-center}
Sample frames from cameras
{: style="text-align: center;"}
{: .small}

Camera frames look pretty straightforward: you can see a tram track on the right with a lonely tram far ahead and some parked cars on the left. Although those road features may seem obvious to detect to you, a computer vision algorithm would struggle to differentiate those by relying solely on the visual data.

## Lidar

The dataset in question contains 114 lidar point cloud frames over duration of 11 seconds. This equals to approximately 10 frames per second, which is a very decent scanning rate, given that we get a 360° field-of-view with each frame containing approximately 120,000 points — a fair amount of data to stream in realtime. Not to clutter the visualizations we will randomly sample 20% of the points for each frame and discard the rest. 

We will additionally visualize _tracklets_, e.g. labeled objects like cars, trams, pedestrians and so on. With a bit of math we will grab information from the KITTI tracklets file and work out each object's bounding box for each frame, feel free to check out the [notebook](https://github.com/navoshta/KITTI-Dataset/blob/master/kitti-dataset.ipynb) for more details. There are only 3 types of objects in this particular 11-seconds piece, we will mark them with bounding boxes as follows: cars will be marked in **blue**, trams in **red** and cyclists in **green**. Let's first visualize a sample lidar frame on a 3D plot.

![image-center]({{ base_path }}/images/posts/kitti-lidar/lidar_frame.png){: .align-center}
Sample lidar frame
{: style="text-align: center;"}
{: .small}

Looks pretty neat! You can see the car with a lidar in the center of a black circle, with laser beams coming out of it. You can even see silhouettes of the cars parked on the left side of the road and tram tracks on the right! And of course bounding boxes for tram and cars, they seem to be exactly where you would expect them looking at the regular camera data. You might have also noticed that only the objects that are visible to the cameras are labeled.

Having this data as a point cloud is extremely useful, as it can be represented in various ways specific to particular applications. You could scale the data points over some particular axis, or simply discard one of the axes to create a plane projection of the point cloud. This is what this velodyne frame would look like when projected on `XZ`, `XY` and `YZ` planes respectively:

![image-center]({{ base_path }}/images/posts/kitti-lidar/lidar_frame_projections.png){: .align-center}
Projections of a sample lidar frame
{: style="text-align: center;"}
{: .small}

Usually you can significantly improve your model performance by preprocessing the data. What you are trying to achieve is a reduction in dimensionality of the input, hoping to extract some useful features and remove those that would be redundant or slow down and confuse the model. In this particular case discarding `Z` coordinate seems like a promising path to explore, as it gives us pretty much a bird's-eye view of the vehicle surroundings. With a more sophisticated feature-engineering coupled with regular camera data as an additional input, you could achieve decent performance on detecting and classifying surrounding objects.

Finally, let's plot all 114 sequential frames and combine them into a short video representing how point cloud changes over time.

![image-center]({{ base_path }}/images/posts/kitti-lidar/pcl_data.gif){: .align-center}
Lidar data plotted over time
{: style="text-align: center;"}
{: .small}

This should give a much better idea of what lidar data looks like. You can clearly see silhouettes of trees and parked cars that our vehicle is passing by — now _that_ would be much easier for an algorithm to interpret. And although lidar is usually used in conjunction with a bunch of other sensors and data sources, it plays a significant role in vehicle [simultaneous localization and mapping](https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping).

<!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/navoshta" data-style="mega" data-count-href="/navoshta/followers" data-count-api="/users/navoshta#followers" data-count-aria-label="# followers on GitHub" aria-label="Follow @navoshta on GitHub">Follow @navoshta</a>
<!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/navoshta/KITTI-Dataset" data-icon="octicon-star" data-style="mega" data-count-href="/navoshta/KITTI-Dataset/stargazers" data-count-api="/repos/navoshta/KITTI-Dataset#stargazers_count" data-count-aria-label="# stargazers on GitHub" aria-label="Star navoshta/KITTI-Dataset on GitHub">Star</a>
<!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/navoshta/KITTI-Dataset/fork" data-icon="octicon-repo-forked" data-style="mega" data-count-href="/navoshta/KITTI-Dataset/network" data-count-api="/repos/navoshta/KITTI-Dataset#forks_count" data-count-aria-label="# forks on GitHub" aria-label="Fork navoshta/KITTI-Dataset on GitHub">Fork</a>
<!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/navoshta/KITTI-Dataset/archive/master.zip" data-icon="octicon-cloud-download" data-style="mega" aria-label="Download navoshta/KITTI-Dataset on GitHub">Download</a>

<!-- Place this tag in your head or just before your close body tag. -->
<script async defer src="https://buttons.github.io/buttons.js"></script>